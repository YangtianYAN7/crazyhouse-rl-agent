# train.py - å¼•å…¥ self-play æœºåˆ¶è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ

import torch
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
import os
from crazyhouse_env import CrazyhouseEnv
from network import ActorCritic
from action_encoder import ALL_POSSIBLE_MOVES
from evaluate import evaluate

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åˆå§‹åŒ–ç¯å¢ƒå’Œæ¨¡å‹
env = CrazyhouseEnv()
input_shape = env.get_observation().shape
n_actions = len(ALL_POSSIBLE_MOVES)

# å½“å‰è®­ç»ƒæ¨¡å‹ï¼ˆç©å®¶1ï¼‰
model = ActorCritic(input_shape, n_actions).to(device)

# å›ºå®šå¯¹æ‰‹æ¨¡å‹ï¼ˆç©å®¶2ï¼‰
opponent_model = ActorCritic(input_shape, n_actions).to(device)
opponent_model.load_state_dict(model.state_dict())  # åˆå§‹åŒ–ä¸ºåŒä¸€æ¨¡å‹
opponent_model.eval()

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

os.makedirs("checkpoints", exist_ok=True)
writer = SummaryWriter(log_dir="runs")

num_episodes = 500
save_opponent_every = 20  # æ¯éš”Nå±€ï¼Œå°†å½“å‰æ¨¡å‹ä¿å­˜ä¸ºå¯¹æ‰‹

for episode in range(num_episodes):
    obs = env.reset()
    done = False
    current_player = 0  # 0 = model, 1 = opponent
    total_reward = 0
    total_loss = 0
    total_actor_loss = 0
    total_critic_loss = 0
    total_entropy = 0

    if episode == 0:
        with open("model_structure.txt", "w") as f:
            f.write(str(model))

    while not done:
        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)

        # ç©å®¶0ï¼šæˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹
        if current_player == 0:
            logits, value = model(obs_tensor)
        else:
            with torch.no_grad():
                logits, _ = opponent_model(obs_tensor)

        # åˆæ³•åŠ¨ä½œ mask
        legal_mask = torch.zeros(n_actions).to(device)
        legal_indices = env.get_legal_action_indices()
        legal_mask[legal_indices] = 1
        masked_logits = logits.masked_fill(legal_mask == 0, float('-inf'))
        probs = torch.softmax(masked_logits, dim=-1)

        if torch.isnan(probs).any() or torch.isinf(probs).any():
            print("âš ï¸ Invalid probs detected, skipping step.")
            break

        dist = torch.distributions.Categorical(probs)
        action = dist.sample()

        # è¿›è¡Œä¸€æ­¥ self-play å¯¹å¼ˆ
        next_obs, reward, done, info = env.step_with_player(action.item(), current_player)

        # åªè®­ç»ƒç©å®¶0çš„æ¨¡å‹
        if current_player == 0:
            next_obs_tensor = torch.tensor(next_obs, dtype=torch.float32).unsqueeze(0).to(device)
            _, next_value = model(next_obs_tensor)

            target = reward + (0.99 * next_value.item() * (1 - int(done)))
            critic_loss = F.mse_loss(value.view(-1), torch.tensor([target], device=device))
            log_prob = torch.log(probs.squeeze(0)[action])
            actor_loss = -log_prob * (target - value.item())
            entropy = dist.entropy()

            loss = actor_loss + critic_loss - 0.01 * entropy

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_actor_loss += actor_loss.item()
            total_critic_loss += critic_loss.item()
            total_entropy += entropy.item()
            total_reward += reward

        obs = next_obs
        current_player = 1 - current_player  # äº¤æ¢ä¸‹æ£‹æ–¹

    # æ—¥å¿—è¾“å‡º
    print(f"ğŸ¯ Episode {episode} | Reward: {total_reward:.2f} | Loss: {total_loss:.4f}")

    writer.add_scalar("Reward", total_reward, episode)
    writer.add_scalar("Loss", total_loss, episode)
    writer.add_scalar("ActorLoss", total_actor_loss, episode)
    writer.add_scalar("CriticLoss", total_critic_loss, episode)
    writer.add_scalar("Entropy", total_entropy, episode)

    # æ¯ 20 å±€æ›´æ–°å¯¹æ‰‹æ¨¡å‹
    if (episode + 1) % save_opponent_every == 0:
        opponent_model.load_state_dict(model.state_dict())

    # æ¯ 100 å±€æ‰“å° top-5 ç­–ç•¥
    if episode % 100 == 0:
        topk = torch.topk(probs.squeeze(0), 5)
        print("ğŸ§  Top-5 Actions:")
        for idx, val in zip(topk.indices.tolist(), topk.values.tolist()):
            print(f"  {ALL_POSSIBLE_MOVES[idx]}: {val:.4f}")

    # æ¯ 500 å±€ä¿å­˜ä¸€æ¬¡
    if episode % 500 == 0:
        ckpt_path = f"checkpoints/model_ep{episode}.pth"
        torch.save(model.state_dict(), ckpt_path)
        print(f"âœ… å·²ä¿å­˜æ¨¡å‹è‡³ {ckpt_path}")

    # æ¯ 5 å±€ä¿å­˜å¹¶è¯„ä¼°
    if (episode + 1) % 5 == 0:
        model_path = f"checkpoints/epoch{episode+1}.pth"
        torch.save(model.state_dict(), model_path)
        print(f"ğŸ“¦ Saved model at {model_path}")
        print(f"ğŸ§ª Evaluating model at epoch {episode+1}...")
        evaluate(model_path, episodes=5)

# æœ€ç»ˆä¿å­˜
torch.save(model.state_dict(), "checkpoints/model.pth")
writer.close()
print("âœ… æ¨¡å‹å·²ä¿å­˜è‡³ checkpoints/model.pth")



























































